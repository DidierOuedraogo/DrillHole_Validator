import streamlit as st
import pandas as pd
import numpy as np
import io
import base64
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from geopy.distance import geodesic
from scipy.spatial import KDTree
import datetime
import os
import sys
import traceback
from functools import partial

# Configuration de la page Streamlit
st.set_page_config(
    page_title="Validation des Donn√©es de Forage Minier",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded"
)

# D√©finition des styles CSS personnalis√©s
def apply_custom_css():
    st.markdown("""
    <style>
        .main {
            background-color: #f5f7f9;
        }
        .stTabs [data-baseweb="tab-panel"] {
            padding-top: 1rem;
        }
        .stAlert {
            border-radius: 8px;
        }
        .stButton>button {
            border-radius: 5px;
            background-color: #4b7bec;
            color: white;
            font-weight: 500;
            border: none;
            padding: 0.5rem 1rem;
        }
        .stButton>button:hover {
            background-color: #3867d6;
        }
        .block-container {
            padding-top: 2rem;
        }
        h1, h2, h3 {
            color: #2c3e50;
        }
        /* Stylisation des tableaux de donn√©es */
        .dataframe {
            border-collapse: collapse;
            margin: 25px 0;
            min-width: 400px;
            box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);
        }
        .dataframe thead tr {
            background-color: #4b7bec;
            color: white;
            text-align: left;
        }
        .dataframe th, .dataframe td {
            padding: 12px 15px;
        }
        .dataframe tbody tr {
            border-bottom: 1px solid #dddddd;
        }
        .dataframe tbody tr:nth-of-type(even) {
            background-color: #f3f3f3;
        }
        .dataframe tbody tr:last-of-type {
            border-bottom: 2px solid #4b7bec;
        }
        /* Badges de statut */
        .success-badge {
            background-color: #5cb85c;
            color: white;
            padding: 4px 8px;
            border-radius: 4px;
            font-size: 14px;
        }
        .warning-badge {
            background-color: #f0ad4e;
            color: white;
            padding: 4px 8px;
            border-radius: 4px;
            font-size: 14px;
        }
        .error-badge {
            background-color: #d9534f;
            color: white;
            padding: 4px 8px;
            border-radius: 4px;
            font-size: 14px;
        }
        /* Section info */
        .info-box {
            background-color: #e8f4f8;
            border-left: 5px solid #4b7bec;
            padding: 1rem;
            margin-bottom: 1rem;
            border-radius: 0 4px 4px 0;
        }
        /* Footer */
        .footer {
            position: relative;
            padding-top: 2rem;
            margin-top: 2rem;
            text-align: center;
            color: #7f8c8d;
            border-top: 1px solid #eee;
        }
    </style>
    """, unsafe_allow_html=True)

apply_custom_css()

# Initialisation des variables de session
if 'files' not in st.session_state:
    st.session_state.files = {}
if 'validation_results' not in st.session_state:
    st.session_state.validation_results = {}
if 'report_data' not in st.session_state:
    st.session_state.report_data = {}

# Titre et description de l'application
st.title("üîç Validation des Donn√©es de Forage Minier")
st.markdown("""
<div class="info-box">
Cette application permet de valider les donn√©es de forage d'exploration mini√®re en v√©rifiant la coh√©rence entre les fichiers,
en d√©tectant les doublons et en s'assurant que les donn√©es respectent les contraintes d√©finies.
</div>
""", unsafe_allow_html=True)

st.markdown("**D√©velopp√© par:** Didier Ouedraogo, P.Geo")

# Fonctions utilitaires
def standardize_column_names(df):
    """Standardise les noms de colonnes"""
    # Dictionnaire de mapping entre diff√©rentes variations de noms de colonnes et noms standardis√©s
    column_mapping = {
        # Identifiants de forage
        'bhid': 'holeid', 'hole_id': 'holeid', 'hole': 'holeid', 'id': 'holeid', 'dhid': 'holeid',
        'drill_hole_id': 'holeid', 'sondage': 'holeid', 'forage': 'holeid',
        
        # Profondeurs
        'depth': 'depth', 'max_depth': 'depth', 'final_depth': 'depth', 'eoh': 'depth',
        'end_of_hole': 'depth', 'length': 'depth', 'longueur': 'depth', 'prof': 'depth',
        'profondeur': 'depth',
        
        # Coordonn√©es
        'xcollar': 'x', 'east': 'x', 'easting': 'x', 'coordx': 'x', 'coord_x': 'x', 'x_utm': 'x',
        'ycollar': 'y', 'north': 'y', 'northing': 'y', 'coordy': 'y', 'coord_y': 'y', 'y_utm': 'y',
        'zcollar': 'z', 'elev': 'z', 'elevation': 'z', 'rl': 'z', 'msl': 'z', 'coordz': 'z',
        'coord_z': 'z', 'z_utm': 'z', 'altitude': 'z',
        
        # Intervalle de d√©but
        'from': 'from', 'start': 'from', 'top': 'from', 'debut': 'from', 'de': 'from',
        'from_m': 'from', 'fr': 'from',
        
        # Intervalle de fin
        'to': 'to', 'end': 'to', 'bottom': 'to', 'fin': 'to', 'a': 'to', '√†': 'to',
        'to_m': 'to', 'jusqua': 'to',
        
        # Azimuth et Dip
        'azimuth': 'azimuth', 'azi': 'azimuth', 'azim': 'azimuth', 'azm': 'azimuth', 'az': 'azimuth',
        'bearing': 'azimuth', 'dir': 'azimuth', 'direction': 'azimuth',
        
        'dip': 'dip', 'plunge': 'dip', 'inclination': 'dip', 'incl': 'dip',
        'pend': 'dip', 'pendage': 'dip',
        
        # Identifiants d'√©chantillons
        'sample_id': 'sampleid', 'sampleno': 'sampleid', 'sample_no': 'sampleid', 'sample': 'sampleid',
        'echantillon': 'sampleid', 'lab_id': 'sampleid',
        
        # Autres
        'latitude': 'latitude', 'lat': 'latitude', 
        'longitude': 'longitude', 'long': 'longitude', 'lng': 'longitude',
        'lithology': 'lithology', 'litho': 'lithology', 'rocktype': 'lithology',
    }
    
    # Convertir tous les noms de colonnes en minuscules
    df.columns = [col.lower() for col in df.columns]
    
    # Appliquer le mapping
    rename_dict = {}
    for col in df.columns:
        if col in column_mapping:
            rename_dict[col] = column_mapping[col]
    
    if rename_dict:
        df = df.rename(columns=rename_dict)
    
    return df

def load_data(file, file_type):
    """Charge et pr√©pare les donn√©es √† partir d'un fichier"""
    if file is None:
        return None
    
    try:
        # D√©tection du type de fichier et chargement
        if file.name.endswith('.csv'):
            try:
                df = pd.read_csv(file, encoding='utf-8')
            except UnicodeDecodeError:
                try:
                    df = pd.read_csv(file, encoding='latin1')
                except Exception as e:
                    st.error(f"Erreur de d√©codage du fichier {file.name}. Essayez d'autres encodages.")
                    return None
        elif file.name.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(file)
        else:
            st.error(f"Format de fichier non pris en charge: {file.name}. Utilisez CSV ou Excel.")
            return None
        
        # Standardisation des noms de colonnes
        df = standardize_column_names(df)
        
        # V√©rification des colonnes essentielles
        essential_columns = {
            'collars': ['holeid'],
            'survey': ['holeid'],
            'assays': ['holeid', 'from', 'to'],
            'litho': ['holeid', 'from', 'to'],
            'density': ['holeid', 'from', 'to'],
            'oxidation': ['holeid', 'from', 'to'],
            'geometallurgy': ['holeid', 'from', 'to'],
            'composites': ['holeid'],
        }
        
        if file_type in essential_columns:
            missing_cols = [col for col in essential_columns[file_type] if col not in df.columns]
            if missing_cols:
                st.error(f"Colonnes manquantes dans {file_type}: {', '.join(missing_cols)}")
                st.write("Colonnes disponibles:", ", ".join(df.columns))
                return None
        
        # V√©rifier les valeurs manquantes dans les colonnes essentielles
        if file_type in essential_columns:
            for col in essential_columns[file_type]:
                if df[col].isnull().sum() > 0:
                    st.warning(f"{df[col].isnull().sum()} valeurs manquantes d√©tect√©es dans la colonne '{col}' du fichier {file_type}")
        
        # Pour les fichiers avec des coordonn√©es spatiales
        if file_type in ['collars', 'composites']:
            coord_columns = ['x', 'y', 'z']
            for col in coord_columns:
                if col in df.columns and not pd.api.types.is_numeric_dtype(df[col]):
                    try:
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                        st.info(f"La colonne '{col}' a √©t√© convertie en type num√©rique.")
                    except Exception as e:
                        st.warning(f"Impossible de convertir la colonne '{col}' en type num√©rique: {str(e)}")
        
        # Pour les fichiers avec des intervalles de profondeur
        if file_type in ['assays', 'litho', 'density', 'oxidation', 'geometallurgy', 'composites']:
            depth_columns = ['from', 'to']
            for col in depth_columns:
                if col in df.columns and not pd.api.types.is_numeric_dtype(df[col]):
                    try:
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                        st.info(f"La colonne '{col}' a √©t√© convertie en type num√©rique.")
                    except Exception as e:
                        st.warning(f"Impossible de convertir la colonne '{col}' en type num√©rique: {str(e)}")
        
        st.success(f"Fichier {file_type} charg√© avec succ√®s: {file.name}")
        return df
    
    except Exception as e:
        st.error(f"Erreur lors du chargement du fichier {file.name}: {str(e)}")
        st.error(traceback.format_exc())
        return None

def download_button(object_to_download, download_filename, button_text):
    """Cr√©e un bouton de t√©l√©chargement pour tout type d'objet"""
    try:
        if isinstance(object_to_download, pd.DataFrame):
            if download_filename.endswith('.csv'):
                object_to_download = object_to_download.to_csv(index=False)
            elif download_filename.endswith(('.xlsx', '.xls')):
                towrite = io.BytesIO()
                object_to_download.to_excel(towrite, index=False)
                towrite.seek(0)
                b64 = base64.b64encode(towrite.read()).decode()
                href = f'<a href="data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64}" download="{download_filename}">{button_text}</a>'
                return href
            else:
                object_to_download = object_to_download.to_csv(index=False)
        elif not isinstance(object_to_download, str):
            object_to_download = str(object_to_download)

        b64 = base64.b64encode(object_to_download.encode()).decode()
        href = f'<a href="data:file/txt;base64,{b64}" download="{download_filename}">{button_text}</a>'
        return href
    except Exception as e:
        return f"Erreur lors de la pr√©paration du t√©l√©chargement : {str(e)}"

def get_status_badge(status):
    """Renvoie le badge HTML correspondant au statut"""
    if status == "success":
        return '<span class="success-badge">‚úì Succ√®s</span>'
    elif status == "warning":
        return '<span class="warning-badge">‚ö† Attention</span>'
    elif status == "error":
        return '<span class="error-badge">‚úó Erreur</span>'
    else:
        return status

def display_status_line(message, status):
    """Affiche une ligne avec un badge de statut"""
    st.markdown(f"{get_status_badge(status)} {message}", unsafe_allow_html=True)

# Fonction pour v√©rifier les doublons d'√©chantillons composites (coordonn√©es proches)
def check_composite_duplicates(data, distance_threshold):
    """V√©rifie les doublons d'√©chantillons composites par proximit√© de coordonn√©es"""
    if data is None or len(data) < 2:
        return {'status': 'info', 'message': "Pas assez de donn√©es pour l'analyse.", 'data': None}
    
    # V√©rifier que les colonnes de coordonn√©es existent
    coord_cols = ['x', 'y', 'z']
    if not all(col in data.columns for col in coord_cols):
        return {
            'status': 'error', 
            'message': f"Colonnes de coordonn√©es manquantes. Colonnes n√©cessaires: {', '.join(coord_cols)}",
            'data': None
        }
    
    # Extraire les coordonn√©es
    try:
        # S'assurer que les coordonn√©es sont num√©riques
        coords = data[coord_cols].copy()
        for col in coord_cols:
            if not pd.api.types.is_numeric_dtype(coords[col]):
                coords[col] = pd.to_numeric(coords[col], errors='coerce')
        
        # Supprimer les lignes avec des valeurs manquantes
        coords.dropna(inplace=True)
        if len(coords) < 2:
            return {'status': 'warning', 'message': "Pas assez de coordonn√©es valides pour l'analyse.", 'data': None}
        
        # Utiliser KDTree pour trouver les paires proches
        tree = KDTree(coords.values)
        pairs = tree.query_pairs(distance_threshold, output_type='ndarray')
        
        if len(pairs) == 0:
            return {'status': 'success', 'message': f"Aucun doublon trouv√© √† moins de {distance_threshold} m√®tres", 'data': None}
        
        # Cr√©er un DataFrame avec les d√©tails des paires trouv√©es
        result_rows = []
        for i, j in pairs:
            i, j = int(i), int(j)
            point1 = coords.iloc[i].values
            point2 = coords.iloc[j].values
            distance = np.linalg.norm(point1 - point2)
            
            # Cr√©er une entr√©e pour chaque paire
            result_rows.append({
                'index1': coords.index[i],
                'index2': coords.index[j],
                'holeid1': data.iloc[i]['holeid'] if 'holeid' in data.columns else f"ID_{i}",
                'holeid2': data.iloc[j]['holeid'] if 'holeid' in data.columns else f"ID_{j}",
                'x1': point1[0],
                'y1': point1[1],
                'z1': point1[2],
                'x2': point2[0],
                'y2': point2[1],
                'z2': point2[2],
                'distance': distance
            })
        
        result_df = pd.DataFrame(result_rows)
        
        # Trier par distance
        result_df.sort_values('distance', inplace=True)
        
        return {
            'status': 'warning',
            'message': f"D√©tection de {len(result_df)} paires d'√©chantillons composites proches (< {distance_threshold} m)",
            'data': result_df
        }
    
    except Exception as e:
        return {'status': 'error', 'message': f"Erreur lors de l'analyse: {str(e)}", 'data': None}

# Fonction pour v√©rifier les forages manquants
def check_missing_holes(collar_data, other_data, file_type):
    """V√©rifie les forages manquants entre datasets"""
    if collar_data is None or other_data is None:
        return {'status': 'info', 'message': "Donn√©es manquantes pour l'analyse", 'data': None}
    
    collar_ids = set(collar_data['holeid'].unique())
    other_ids = set(other_data['holeid'].unique())
    
    # Forages dans collar mais absents dans l'autre fichier
    missing_in_other = collar_ids - other_ids
    
    # Forages dans l'autre fichier mais absents dans collar
    extra_in_other = other_ids - collar_ids
    
    result = {
        'missing_in_other': list(missing_in_other),
        'extra_in_other': list(extra_in_other)
    }
    
    # D√©terminer le statut
    if missing_in_other and extra_in_other:
        status = 'error'
    elif missing_in_other or extra_in_other:
        status = 'warning'
    else:
        status = 'success'
    
    return {
        'status': status,
        'message': f"Analyse des forages entre collars et {file_type}",
        'data': result
    }

# Fonction pour v√©rifier les doublons
def check_duplicates(data, file_type):
    """V√©rifie les doublons dans un dataset"""
    if data is None:
        return {'status': 'info', 'message': "Donn√©es manquantes pour l'analyse", 'data': None}
    
    result = None
    
    if file_type == 'collars':
        # V√©rifier les doublons de forages dans collars
        duplicates = data[data.duplicated(subset=['holeid'], keep=False)]
        if not duplicates.empty:
            status = 'error'
            message = f"Doublons de forages d√©tect√©s dans collars: {len(duplicates)} enregistrements"
        else:
            status = 'success'
            message = "Aucun doublon de forage d√©tect√© dans collars"
        result = duplicates
    
    elif file_type in ['assays', 'litho', 'density', 'oxidation', 'geometallurgy']:
        # V√©rifier les doublons d'intervalles
        if all(col in data.columns for col in ['holeid', 'from', 'to']):
            duplicates = data[data.duplicated(subset=['holeid', 'from', 'to'], keep=False)]
            if not duplicates.empty:
                status = 'error'
                message = f"Doublons d'intervalles d√©tect√©s dans {file_type}: {len(duplicates)} enregistrements"
            else:
                status = 'success'
                message = f"Aucun doublon d'intervalle d√©tect√© dans {file_type}"
            result = duplicates
        else:
            status = 'warning'
            message = f"Impossible de v√©rifier les doublons d'intervalles dans {file_type}: colonnes requises manquantes"
    
    elif file_type == 'survey':
        # V√©rifier les doublons dans les mesures de survey
        if all(col in data.columns for col in ['holeid', 'depth']):
            duplicates = data[data.duplicated(subset=['holeid', 'depth'], keep=False)]
            if not duplicates.empty:
                status = 'error'
                message = f"Doublons de mesures d√©tect√©s dans {file_type}: {len(duplicates)} enregistrements"
            else:
                status = 'success'
                message = f"Aucun doublon de mesure d√©tect√© dans {file_type}"
            result = duplicates
        else:
            status = 'warning'
            message = f"Impossible de v√©rifier les doublons dans {file_type}: colonnes requises manquantes"
    
    # V√©rifier les doublons d'√©chantillons si la colonne sampleid existe
    if 'sampleid' in data.columns:
        sample_duplicates = data[data.duplicated(subset=['sampleid'], keep=False)]
        if not sample_duplicates.empty:
            status = 'error'
            message += f" | Doublons d'identifiants d'√©chantillons d√©tect√©s: {len(sample_duplicates)} enregistrements"
            if result is None:
                result = sample_duplicates
            else:
                result = pd.concat([result, sample_duplicates]).drop_duplicates()
    
    return {
        'status': status,
        'message': message,
        'data': result
    }

# Fonction pour v√©rifier les intervalles de profondeur
def check_interval_depths(collar_data, interval_data, file_type, tolerance=0.1):
    """V√©rifie les intervalles de profondeur par rapport aux profondeurs maximales des forages"""
    if collar_data is None or interval_data is None:
        return {'status': 'info', 'message': "Donn√©es manquantes pour l'analyse", 'data': None}
    
    if 'depth' not in collar_data.columns:
        return {'status': 'error', 'message': "Colonne 'depth' manquante dans les donn√©es de collars", 'data': None}
    
    if not all(col in interval_data.columns for col in ['holeid', 'from', 'to']):
        return {'status': 'error', 'message': "Colonnes requises ('holeid', 'from', 'to') manquantes dans les donn√©es d'intervalle", 'data': None}
    
    # Cr√©er un dictionnaire des profondeurs maximales pour chaque forage
    max_depths = dict(zip(collar_data['holeid'], collar_data['depth']))
    
    # V√©rifier chaque intervalle
    issues = []
    for idx, row in interval_data.iterrows():
        hole_id = row['holeid']
        from_depth = row['from']
        to_depth = row['to']
        
        # V√©rifier si le forage existe dans collar
        if hole_id in max_depths:
            max_depth = max_depths[hole_id]
            
            # V√©rifier si les profondeurs sont valides
            if to_depth > max_depth + tolerance:
                issues.append({
                    'index': idx,
                    'holeid': hole_id,
                    'from': from_depth,
                    'to': to_depth,
                    'max_depth': max_depth,
                    'issue': f"La profondeur 'to' ({to_depth}) d√©passe la profondeur maximale du forage ({max_depth})"
                })
            
            if from_depth > to_depth:
                issues.append({
                    'index': idx,
                    'holeid': hole_id,
                    'from': from_depth,
                    'to': to_depth,
                    'max_depth': max_depth,
                    'issue': f"La profondeur 'from' ({from_depth}) est sup√©rieure √† 'to' ({to_depth})"
                })
    
    results_df = pd.DataFrame(issues) if issues else pd.DataFrame()
    
    if len(issues) > 0:
        status = 'error'
        message = f"Probl√®mes d'intervalles d√©tect√©s dans {file_type}: {len(issues)} intervalles probl√©matiques"
    else:
        status = 'success'
        message = f"Aucun probl√®me d'intervalle d√©tect√© dans {file_type}"
    
    return {
        'status': status,
        'message': message,
        'data': results_df
    }

# Interface utilisateur avec onglets
import_tab, validate_tab, report_tab = st.tabs(["Import de Donn√©es", "Validation", "Rapport"])

with import_tab:
    st.header("Import des fichiers de donn√©es")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Fichier des collars (obligatoire)")
        collar_file = st.file_uploader("S√©lectionner le fichier collars (CSV, Excel)", type=["csv", "xlsx", "xls"], key="collar_uploader")
        if collar_file is not None:
            collar_data = load_data(collar_file, 'collars')
            if collar_data is not None:
                st.session_state.files['collars'] = collar_data
                st.dataframe(collar_data.head())
                st.info(f"Total des forages: {len(collar_data)}")
    
    with col2:
        st.subheader("Fichier des surveys")
        survey_file = st.file_uploader("S√©lectionner le fichier survey (CSV, Excel)", type=["csv", "xlsx", "xls"], key="survey_uploader")
        if survey_file is not None:
            survey_data = load_data(survey_file, 'survey')
            if survey_data is not None:
                st.session_state.files['survey'] = survey_data
                st.dataframe(survey_data.head())
                st.info(f"Total des enregistrements survey: {len(survey_data)}")
    
    st.subheader("Donn√©es d'intervalles et d'√©chantillons")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        assay_file = st.file_uploader("Fichier d'analyses (assays)", type=["csv", "xlsx", "xls"], key="assay_uploader")
        if assay_file is not None:
            assay_data = load_data(assay_file, 'assays')
            if assay_data is not None:
                st.session_state.files['assays'] = assay_data
                st.dataframe(assay_data.head())
                st.info(f"Total des √©chantillons d'analyses: {len(assay_data)}")
    
    with col2:
        litho_file = st.file_uploader("Fichier de lithologie", type=["csv", "xlsx", "xls"], key="litho_uploader")
        if litho_file is not None:
            litho_data = load_data(litho_file, 'litho')
            if litho_data is not None:
                st.session_state.files['litho'] = litho_data
                st.dataframe(litho_data.head())
                st.info(f"Total des intervalles de lithologie: {len(litho_data)}")
    
    with col3:
        density_file = st.file_uploader("Fichier de densit√©", type=["csv", "xlsx", "xls"], key="density_uploader")
        if density_file is not None:
            density_data = load_data(density_file, 'density')
            if density_data is not None:
                st.session_state.files['density'] = density_data
                st.dataframe(density_data.head())
                st.info(f"Total des mesures de densit√©: {len(density_data)}")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        oxidation_file = st.file_uploader("Fichier d'oxydation", type=["csv", "xlsx", "xls"], key="oxidation_uploader")
        if oxidation_file is not None:
            oxidation_data = load_data(oxidation_file, 'oxidation')
            if oxidation_data is not None:
                st.session_state.files['oxidation'] = oxidation_data
                st.dataframe(oxidation_data.head())
                st.info(f"Total des intervalles d'oxydation: {len(oxidation_data)}")
    
    with col2:
        geomet_file = st.file_uploader("Fichier de g√©om√©tallurgie", type=["csv", "xlsx", "xls"], key="geomet_uploader")
        if geomet_file is not None:
            geomet_data = load_data(geomet_file, 'geometallurgy')
            if geomet_data is not None:
                st.session_state.files['geometallurgy'] = geomet_data
                st.dataframe(geomet_data.head())
                st.info(f"Total des intervalles g√©om√©tallurgiques: {len(geomet_data)}")
    
    with col3:
        composite_file = st.file_uploader("Fichier de composites", type=["csv", "xlsx", "xls"], key="composite_uploader")
        if composite_file is not None:
            composite_data = load_data(composite_file, 'composites')
            if composite_data is not None:
                st.session_state.files['composites'] = composite_data
                st.dataframe(composite_data.head())
                st.info(f"Total des composites: {len(composite_data)}")

with validate_tab:
    st.header("Validation des donn√©es")
    
    # Param√®tres de validation dans la barre lat√©rale
    st.sidebar.header("Param√®tres de validation")
    depth_tolerance = st.sidebar.number_input("Tol√©rance de profondeur (m√®tres)", min_value=0.0, max_value=1.0, value=0.1, step=0.01)
    distance_threshold = st.sidebar.number_input("Seuil de distance pour composites proches (m√®tres)", min_value=0.01, max_value=10.0, value=1.0, step=0.1)
    
    # V√©rifier si le fichier collars est pr√©sent
    if 'collars' not in st.session_state.files:
        st.warning("Veuillez d'abord importer le fichier des collars dans l'onglet 'Import de donn√©es'.")
    else:
        if st.button("üîç Lancer la validation compl√®te", use_container_width=True):
            st.session_state.validation_results = {}
            st.session_state.report_data = {
                'timestamp': datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'files_analyzed': list(st.session_state.files.keys()),
                'validation_results': {},
                'summary': {}
            }
            
            with st.spinner("Validation en cours..."):
                collar_data = st.session_state.files['collars']
                
                # Analyse 1: Doublons dans les fichiers
                st.subheader("V√©rification des doublons")
                
                for file_type, file_data in st.session_state.files.items():
                    result = check_duplicates(file_data, file_type)
                    st.session_state.validation_results[f'duplicates_{file_type}'] = result
                    display_status_line(result['message'], result['status'])
                    
                    # Ajouter au rapport
                    st.session_state.report_data['validation_results'][f'duplicates_{file_type}'] = {
                        'status': result['status'],
                        'message': result['message'],
                        'count': len(result['data']) if result['data'] is not None else 0
                    }
                
                # Analyse 2: V√©rification des forages manquants
                st.subheader("V√©rification des forages manquants")
                
                for file_type, file_data in st.session_state.files.items():
                    if file_type != 'collars':
                        result = check_missing_holes(collar_data, file_data, file_type)
                        st.session_state.validation_results[f'missing_holes_{file_type}'] = result
                        
                        if result['status'] == 'success':
                            st.success(f"Tous les forages dans {file_type} correspondent aux collars")
                        else:
                            if result['data']['missing_in_other']:
                                st.warning(f"{len(result['data']['missing_in_other'])} forages dans collars sont absents de {file_type}")
                            
                            if result['data']['extra_in_other']:
                                st.error(f"{len(result['data']['extra_in_other'])} forages dans {file_type} sont absents de collars")
                        
                        # Ajouter au rapport
                        st.session_state.report_data['validation_results'][f'missing_holes_{file_type}'] = {
                            'status': result['status'],
                            'message': result['message'],
                            'missing_count': len(result['data']['missing_in_other']) if result['data'] else 0,
                            'extra_count': len(result['data']['extra_in_other']) if result['data'] else 0
                        }
                
                # Analyse 3: V√©rification des intervalles de profondeur
                st.subheader("V√©rification des intervalles de profondeur")
                
                interval_types = ['assays', 'litho', 'density', 'oxidation', 'geometallurgy', 'composites']
                
                for file_type in interval_types:
                    if file_type in st.session_state.files:
                        result = check_interval_depths(collar_data, st.session_state.files[file_type], file_type, depth_tolerance)
                        st.session_state.validation_results[f'interval_depths_{file_type}'] = result
                        
                        display_status_line(result['message'], result['status'])
                        
                        if result['status'] == 'error':
                            with st.expander("Voir les d√©tails des probl√®mes d'intervalles"):
                                st.dataframe(result['data'])
                        
                        # Ajouter au rapport
                        st.session_state.report_data['validation_results'][f'interval_depths_{file_type}'] = {
                            'status': result['status'],
                            'message': result['message'],
                            'count': len(result['data']) if result['data'] is not None else 0
                        }
                
                # Analyse 4: V√©rification des composites proches
                if 'composites' in st.session_state.files:
                    st.subheader("V√©rification des composites proches")
                    
                    result = check_composite_duplicates(st.session_state.files['composites'], distance_threshold)
                    st.session_state.validation_results['composite_duplicates'] = result
                    
                    display_status_line(result['message'], result['status'])
                    
                    if result['status'] == 'warning' and result['data'] is not None:
                        with st.expander("Voir les d√©tails des composites proches"):
                            st.dataframe(result['data'])
                    
                    # Ajouter au rapport
                    st.session_state.report_data['validation_results']['composite_duplicates'] = {
                        'status': result['status'],
                        'message': result['message'],
                        'count': len(result['data']) if result['data'] is not None else 0
                    }
                
                # Pr√©paration du r√©sum√© de validation
                success_count = sum(1 for result in st.session_state.validation_results.values() if result['status'] == 'success')
                warning_count = sum(1 for result in st.session_state.validation_results.values() if result['status'] == 'warning')
                error_count = sum(1 for result in st.session_state.validation_results.values() if result['status'] == 'error')
                
                st.session_state.report_data['summary'] = {
                    'total_validations': len(st.session_state.validation_results),
                    'success_count': success_count,
                    'warning_count': warning_count,
                    'error_count': error_count
                }
                
                st.success(f"Validation termin√©e: {success_count} succ√®s, {warning_count} avertissements, {error_count} erreurs")
        
        # Affichage des r√©sultats de validation pr√©c√©dents
        if st.session_state.validation_results:
            st.subheader("R√©sultats de validation existants")
            
            # Option pour afficher tous les d√©tails
            show_details = st.checkbox("Afficher tous les d√©tails des r√©sultats")
            
            for validation_type, result in st.session_state.validation_results.items():
                display_status_line(result['message'], result['status'])
                
                if show_details and result['data'] is not None:
                    if isinstance(result['data'], pd.DataFrame) and not result['data'].empty:
                        st.dataframe(result['data'])
                    elif isinstance(result['data'], dict):
                        st.write(result['data'])
            
            # Bouton pour exporter les r√©sultats
            st.subheader("Exportation des r√©sultats")
            
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("Exporter le rapport de validation (CSV)"):
                    # Cr√©er un DataFrame pour le rapport
                    report_rows = []
                    
                    for validation_type, result in st.session_state.validation_results.items():
                        row = {
                            'validation_type': validation_type,
                            'status': result['status'],
                            'message': result['message'],
                            'details': str(result['data']) if result['data'] is not None else 'Aucun d√©tail'
                        }
                        report_rows.append(row)
                    
                    report_df = pd.DataFrame(report_rows)
                    
                    # G√©n√©rer le lien de t√©l√©chargement
                    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                    csv_filename = f"rapport_validation_{timestamp}.csv"
                    
                    csv_href = download_button(report_df, csv_filename, "T√©l√©charger le rapport CSV")
                    st.markdown(csv_href, unsafe_allow_html=True)
            
            with col2:
                if st.button("Exporter les r√©sultats d√©taill√©s (Excel)"):
                    # Cr√©er un fichier Excel avec une feuille pour chaque validation
                    output = io.BytesIO()
                    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                        # D'abord, √©crire un r√©sum√©
                        summary_data = {
                            'Type de validation': [],
                            'Statut': [],
                            'Message': [],
                            'Nombre de probl√®mes': []
                        }
                        
                        for validation_type, result in st.session_state.validation_results.items():
                            summary_data['Type de validation'].append(validation_type)
                            summary_data['Statut'].append(result['status'])
                            summary_data['Message'].append(result['message'])
                            
                            if result['data'] is None:
                                count = 0
                            elif isinstance(result['data'], pd.DataFrame):
                                count = len(result['data'])
                            elif isinstance(result['data'], dict) and 'missing_in_other' in result['data']:
                                count = len(result['data']['missing_in_other']) + len(result['data']['extra_in_other'])
                            else:
                                count = 0
                            
                            summary_data['Nombre de probl√®mes'].append(count)
                        
                        pd.DataFrame(summary_data).to_excel(writer, sheet_name='R√©sum√©', index=False)
                        
                        # Ensuite, √©crire les d√©tails de chaque validation
                        for validation_type, result in st.session_state.validation_results.items():
                            if result['data'] is not None:
                                if isinstance(result['data'], pd.DataFrame) and not result['data'].empty:
                                    # Limiter le nom de la feuille √† 31 caract√®res (limite Excel)
                                    sheet_name = validation_type[:31]
                                    result['data'].to_excel(writer, sheet_name=sheet_name, index=False)
                                elif isinstance(result['data'], dict) and ('missing_in_other' in result['data'] or 'extra_in_other' in result['data']):
                                    missing = pd.DataFrame({'HOLEID': result['data'].get('missing_in_other', []), 'Status': 'Missing'})
                                    extra = pd.DataFrame({'HOLEID': result['data'].get('extra_in_other', []), 'Status': 'Extra'})
                                    pd.concat([missing, extra]).to_excel(writer, sheet_name=validation_type[:31], index=False)
                    
                    # G√©n√©rer le lien de t√©l√©chargement
                    output.seek(0)
                    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                    excel_filename = f"resultats_validation_detailles_{timestamp}.xlsx"
                    
                    b64 = base64.b64encode(output.getvalue()).decode()
                    excel_href = f'<a href="data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64}" download="{excel_filename}">T√©l√©charger les r√©sultats Excel</a>'
                    st.markdown(excel_href, unsafe_allow_html=True)

with report_tab:
    st.header("Rapport de validation")
    
    if not st.session_state.report_data:
        st.info("Veuillez d'abord lancer la validation dans l'onglet 'Validation' pour g√©n√©rer un rapport.")
    else:
        # R√©cup√©rer les donn√©es du rapport
        report = st.session_state.report_data
        
        # En-t√™te du rapport
        st.subheader("R√©sum√© de la validation")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Validations totales", report['summary']['total_validations'])
        
        with col2:
            st.metric("Succ√®s", report['summary']['success_count'], delta=f"{report['summary']['success_count']/report['summary']['total_validations']*100:.0f}%")
        
        with col3:
            st.metric("Avertissements", report['summary']['warning_count'], delta=f"{report['summary']['warning_count']/report['summary']['total_validations']*100:.0f}%")
        
        with col4:
            st.metric("Erreurs", report['summary']['error_count'], delta=f"{report['summary']['error_count']/report['summary']['total_validations']*100:.0f}%")
        
        # Statut global
        if report['summary']['error_count'] > 0:
            st.error("‚ö†Ô∏è Le jeu de donn√©es pr√©sente des erreurs importantes qui doivent √™tre corrig√©es.")
        elif report['summary']['warning_count'] > 0:
            st.warning("‚ö†Ô∏è Le jeu de donn√©es pr√©sente quelques probl√®mes mineurs qui pourraient n√©cessiter votre attention.")
        else:
            st.success("‚úÖ Le jeu de donn√©es a pass√© toutes les validations avec succ√®s!")
        
        # R√©sultats d√©taill√©s sous forme de tableau
        st.subheader("D√©tails des validations")
        
        # Cr√©er un DataFrame pour le rapport
        report_rows = []
        
        for validation_type, result in report['validation_results'].items():
            row = {
                'Type de validation': validation_type.replace('_', ' ').title(),
                'Statut': result['status'].upper(),
                'Message': result['message'],
                'Probl√®mes d√©tect√©s': result.get('count', 0) + result.get('missing_count', 0) + result.get('extra_count', 0)
            }
            report_rows.append(row)
        
        report_df = pd.DataFrame(report_rows)
        st.dataframe(report_df, use_container_width=True)
        
        # Visualisation des statistiques
        st.subheader("Visualisation des r√©sultats")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Graphique des statuts
            status_counts = {
                'Succ√®s': report['summary']['success_count'],
                'Avertissements': report['summary']['warning_count'],
                'Erreurs': report['summary']['error_count']
            }
            
            status_df = pd.DataFrame({
                'Statut': list(status_counts.keys()),
                'Nombre': list(status_counts.values())
            })
            
            fig = px.pie(status_df, values='Nombre', names='Statut', 
                         title='R√©partition des statuts de validation',
                         color='Statut',
                         color_discrete_map={
                             'Succ√®s': '#5cb85c',
                             'Avertissements': '#f0ad4e',
                             'Erreurs': '#d9534f'
                         })
            fig.update_traces(textposition='inside', textinfo='percent+label')
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Graphique des probl√®mes par type de validation
            problems_df = report_df.sort_values('Probl√®mes d√©tect√©s', ascending=False)
            
            fig = px.bar(problems_df, x='Type de validation', y='Probl√®mes d√©tect√©s',
                         title='Nombre de probl√®mes par type de validation',
                         color='Statut',
                         color_discrete_map={
                             'SUCCESS': '#5cb85c',
                             'WARNING': '#f0ad4e',
                             'ERROR': '#d9534f'
                         })
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
        
        # Distribution des profondeurs de forage si disponible
        if 'collars' in st.session_state.files and 'depth' in st.session_state.files['collars'].columns:
            st.subheader("Distribution des profondeurs de forage")
            
            collar_depths = st.session_state.files['collars']['depth'].dropna()
            
            fig = px.histogram(collar_depths, nbins=30,
                              labels={'value': 'Profondeur (m)', 'count': 'Nombre de forages'},
                              title='Distribution des profondeurs de forage')
            fig.update_layout(bargap=0.1)
            st.plotly_chart(fig, use_container_width=True)
        
        # Afficher la distribution spatiale des forages si les coordonn√©es sont disponibles
        if 'collars' in st.session_state.files and all(col in st.session_state.files['collars'].columns for col in ['x', 'y']):
            st.subheader("Distribution spatiale des forages")
            
            collars = st.session_state.files['collars']
            
            fig = px.scatter(collars, x='x', y='y', 
                            hover_name='holeid' if 'holeid' in collars.columns else None,
                            hover_data=['depth'] if 'depth' in collars.columns else None,
                            color='depth' if 'depth' in collars.columns else None,
                            title='Carte des emplacements de forage')
            
            fig.update_layout(
                autosize=True,
                height=600,
                margin=dict(l=50, r=50, b=100, t=100, pad=4),
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        # Exportation du rapport complet
        st.subheader("Exporter le rapport complet")
        
        if st.button("G√©n√©rer un rapport complet (HTML)"):
            # Cr√©er le contenu HTML
            html_content = f"""
            <!DOCTYPE html>
            <html>
            <head>
                <title>Rapport de Validation des Donn√©es de Forage Minier</title>
                <style>
                    body {{ font-family: Arial, sans-serif; margin: 0; padding: 0; color: #333; }}
                    .container {{ max-width: 1200px; margin: 0 auto; padding: 20px; }}
                    header {{ background-color: #2c3e50; color: white; padding: 20px; margin-bottom: 30px; }}
                    h1, h2, h3 {{ margin-top: 30px; }}
                    .summary {{ display: flex; justify-content: space-between; margin: 20px 0; }}
                    .summary-card {{ padding: 15px; border-radius: 5px; width: 22%; text-align: center; }}
                    .success {{ background-color: #dff0d8; color: #3c763d; }}
                    .warning {{ background-color: #fcf8e3; color: #8a6d3b; }}
                    .error {{ background-color: #f2dede; color: #a94442; }}
                    .info {{ background-color: #d9edf7; color: #31708f; }}
                    table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
                    th, td {{ padding: 10px; border: 1px solid #ddd; text-align: left; }}
                    th {{ background-color: #f2f2f2; }}
                    tr:nth-child(even) {{ background-color: #f9f9f9; }}
                    .status-badge {{ padding: 5px 10px; border-radius: 3px; font-weight: bold; }}
                    .footer {{ margin-top: 50px; text-align: center; color: #777; font-size: 14px; }}
                </style>
            </head>
            <body>
                <header>
                    <h1>Rapport de Validation des Donn√©es de Forage Minier</h1>
                    <p>Date de g√©n√©ration: {report['timestamp']}</p>
                </header>
                
                <div class="container">
                    <h2>R√©sum√© de la validation</h2>
                    
                    <div class="summary">
                        <div class="summary-card info">
                            <h3>{report['summary']['total_validations']}</h3>
                            <p>Validations totales</p>
                        </div>
                        <div class="summary-card success">
                            <h3>{report['summary']['success_count']}</h3>
                            <p>Succ√®s</p>
                        </div>
                        <div class="summary-card warning">
                            <h3>{report['summary']['warning_count']}</h3>
                            <p>Avertissements</p>
                        </div>
                        <div class="summary-card error">
                            <h3>{report['summary']['error_count']}</h3>
                            <p>Erreurs</p>
                        </div>
                    </div>
                    
                    <h2>Fichiers analys√©s</h2>
                    <ul>
            """
            
            for file in report['files_analyzed']:
                html_content += f"<li>{file}</li>"
            
            html_content += """
                    </ul>
                    
                    <h2>D√©tails des validations</h2>
                    <table>
                        <tr>
                            <th>Type de validation</th>
                            <th>Statut</th>
                            <th>Message</th>
                            <th>Probl√®mes d√©tect√©s</th>
                        </tr>
            """
            
            for validation_type, result in report['validation_results'].items():
                status_class = result['status']
                problem_count = result.get('count', 0) + result.get('missing_count', 0) + result.get('extra_count', 0)
                
                html_content += f"""
                        <tr>
                            <td>{validation_type.replace('_', ' ').title()}</td>
                            <td><span class="status-badge {status_class}">{result['status'].upper()}</span></td>
                            <td>{result['message']}</td>
                            <td>{problem_count}</td>
                        </tr>
                """
            
            html_content += """
                    </table>
                    
                    <h2>Conclusion</h2>
            """
            
            if report['summary']['error_count'] > 0:
                html_content += """
                    <div class="error" style="padding: 15px; border-radius: 5px;">
                        <p>‚ö†Ô∏è Le jeu de donn√©es pr√©sente des erreurs importantes qui doivent √™tre corrig√©es.</p>
                    </div>
                """
            elif report['summary']['warning_count'] > 0:
                html_content += """
                    <div class="warning" style="padding: 15px; border-radius: 5px;">
                        <p>‚ö†Ô∏è Le jeu de donn√©es pr√©sente quelques probl√®mes mineurs qui pourraient n√©cessiter votre attention.</p>
                    </div>
                """
            else:
                html_content += """
                    <div class="success" style="padding: 15px; border-radius: 5px;">
                        <p>‚úÖ Le jeu de donn√©es a pass√© toutes les validations avec succ√®s!</p>
                    </div>
                """
            
            html_content += """
                    <div class="footer">
                        <p>G√©n√©r√© par l'application de Validation des Donn√©es de Forage Minier</p>
                        <p>D√©velopp√© par Didier Ouedraogo, P.Geo</p>
                    </div>
                </div>
            </body>
            </html>
            """
            
            # G√©n√©rer le lien de t√©l√©chargement
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            html_filename = f"rapport_validation_complet_{timestamp}.html"
            
            b64 = base64.b64encode(html_content.encode()).decode()
            html_href = f'<a href="data:text/html;base64,{b64}" download="{html_filename}">T√©l√©charger le rapport HTML complet</a>'
            st.markdown(html_href, unsafe_allow_html=True)

# Pied de page
st.markdown("""
<div class="footer">
    <p>¬© 2025 Didier Ouedraogo, P.Geo | Application de validation de donn√©es de forage minier</p>
</div>
""", unsafe_allow_html=True)